---
layout: article
tags: DeepLearning
---

## 正则化

> 模型越复杂越容易出现**过拟合**状态，所以需要一种机制来保证我们模型的“简单”，这样我们的模型才能有较好的泛化能力，正则化是这类机制之一。

欧几里得范数：$||w||_2^2 = \sum_{j=1}^{n_x}w_j^2 =  w^Tw$

L2范数：$J(w,b) = \frac{1}{m} *L(y, \hat{y}) + \frac{\lambda}{2m}||w||_2^2$

L1范数：$\frac{\lambda}{2m} \sum_{j=1}^{u_x}|w_j|=\frac{\lambda}{2m}||w||_1$

![paste image](http://cdn.lisongqian.cn/15307551919108rxdn1vo.png?imageslim)

#### 推导过程

> [泰勒公式](https://baike.baidu.com/item/%E6%B3%B0%E5%8B%92%E5%85%AC%E5%BC%8F)  $f(x) = \frac{f(a)}{0!}+\frac{f'(a)}{1!}(x-a)+\frac{f''(a)}{2!}(x-a)^2+...+\frac{f^{(n)}(a)}{n!}(x-a)^n +R_n(x)$

#### 为什么可以减少过拟合

$$J(w^{[\ell]},b^{[\ell]}) = \frac{1}{m} *L(y^i, \hat{y}^i) + \frac{\lambda}{2m}||w^{[\ell]}||_2^2$$

直观理解就是$\lambda$增加到足够大，$w$会趋近于0，但实际上是不会发生这种情况。通过正则化方法来消除或减少大量隐藏单元的影响，使这个网络变的更简单，越来越接近逻辑回归，**在直觉上认为大量隐藏单元被完全消除了**，但实际上是所有隐藏单元依然存在，但是他们的影响变小了。

#### 正则化结果

如果正则化参数很大，$w^{[\ell]}$很小，$z$ 将会相对变小。由于$z$的取值范围很小，会导致激活函数相对呈线性，整个神经网络会计算离线性函数近的值，不会成为一个极复杂的高度非线性函数，不会发生过拟合。

## Dropout正则化

Dropout正则化是随机减少每层的神经元来实现的。

#### Dropout方法——Inverted Dropout

1. 定义一个向量$d$，$d3$表示一个三层的Dropout向量，$keep-prob$ 表示保留某个隐藏单元的概率：

   ``` d3=np.random.rand(a3.shape[0],a3.shape[1])<keep-prob```

2. 从第三层中获取激活函数($d$的值变为了0或1，从而实现随机移除神经元)，这里叫做$a3$：

   ``` a3=np.multply(a3,d3)  # a3*=d3```  

3. 由于我们随机的移除了一些节点，这样最终的预测值会发生改变，所以在这一步骤中对期望值进行补偿：

   ```a/=keep-prob```

在测试集中不需要使用Dropout。

## 其他正则化方法

- 数据扩增（data augmentation）
- 提前停止训练神经网络（early stopping）

##### REFERENCES

- [神经网络中使用正则化减少方差](http://www.niuxuewei.com/2018/04/07/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%AD%E4%BD%BF%E7%94%A8%E6%AD%A3%E5%88%99%E5%8C%96%E5%87%8F%E5%B0%91%E6%96%B9%E5%B7%AE/)
- [机器学习中常常提到的正则化到底是什么意思](https://www.zhihu.com/question/20924039)