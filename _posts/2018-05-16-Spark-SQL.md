---
layout: article
tags: SQL
description: SparkSQL
---

# Spark SQL

## 启动Spark Shell:

```shell
lisongqian@master:/opt/spark$ ./bin/spark-shell
```

<!--more-->

## 操作DataFrames

在Spark Shell启动时，会输出`Spark context available as 'sc'`，这意味着spark-shell已经初始化了一个可用的spark context叫做sc.

#### JSON：

```json
{"name":"Michael"}
{"name":"Andy", "age":30}
{"name":"Justin", "age":19}
```

#### 导入数据源

```scala
scala> val sqlContext = new org.apache.spark.sql.SQLContext(sc)
scala> import sqlContext.implicits._
scala> val df = sqlContext.read.json("file:///opt/spark/examples/src/main/resources/people.json")
```

#### 输出数据源

```scala
scala> df.show() // 输出数据源内容
scala> df.select("name").show()    // 只显示 "name" 列
scala> df.select(df("name"), df("age") + 1).show()   // 将 "age" 加 1
scala> df.filter(df("age") > 21).show()     # 条件语句
scala> df.groupBy("age").count().show()   // groupBy 操作
```

#### SQL语句操作

```scala
scala> df.registerTempTable("people")     // 将 DataFrame 注册为临时表 people
scala> val result = sqlContext.sql("SELECT name, age FROM people WHERE age >= 13 AND age <= 19")  // 执行 SQL 查询
scala> result.show()
```

